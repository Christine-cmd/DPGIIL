{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import abstractmethod\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "from itertools import cycle\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "# import pytorch_lightning as pl\n",
    "# from pytorch_lightning import Trainer\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "# from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, Normalizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, adjusted_rand_score, normalized_mutual_info_score, homogeneity_score,  completeness_score, v_measure_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "import onlineVDP_suff_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise RuntimeWarning()\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_VAE(BaseVAE):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 dpmm_param: dict,\n",
    "                 hidden_layers: list,\n",
    "                 output_type: str='linear',\n",
    "                 **kwargs) -> None:\n",
    "        \n",
    "        super(DP_VAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.dpmm_param = dpmm_param\n",
    "        self.output_type = output_type\n",
    "        # the parameters of the DPGMM model\n",
    "        self.current_result = {}\n",
    "        self.current_suff_stat = {}\n",
    "        self.opts = onlineVDP_suff_stat.mkopts_vdp(**dpmm_param)\n",
    "        print(\"Log: opts\", self.opts)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers[0], hidden_layers[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers[1], hidden_layers[2]),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_layers[2], latent_dim)\n",
    "        self.fc_log_var = nn.Linear(hidden_layers[2], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, hidden_layers[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers[2], hidden_layers[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers[1], hidden_layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layers[0], self.input_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network and returns the latent codes.\n",
    "        param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_log_var(result)\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes onto the image space.\n",
    "        param z: (Tensor) [B x D]\n",
    "        return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.decoder(z)\n",
    "\n",
    "        if self.output_type == 'linear':\n",
    "            pass\n",
    "        elif self.output_type == 'sigmoid':\n",
    "            result = torch.sigmoid(result)\n",
    "        else: # tahn\n",
    "            result = torch.tanh(result)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def reparameterize(self, mu: Tensor, log_var: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu  \n",
    "      \n",
    "    # Forward propagation: inference network outputs mu(x) and log_var(x) given an input x, then use repameterization trick to sample a batch of z, which are feed to DPGMM model\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  [self.decode(z), input, mu, log_var, z] # [recon, input, mu, log_var, z]\n",
    "\n",
    "    def reconstruction(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input sample x, returns the reconstructed sample\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "    def fit_dpmm(self, z):\n",
    "        # use the lanter variables z of the entire dataset to fit the DPGMM model\n",
    "        # for the DPGMM, the dimension of input is D*N, while for the VAE, the dimension of input is N*D\n",
    "        z = z.detach().cpu().t()\n",
    "        init_prior = onlineVDP_suff_stat.mk_hp_prior(z, **self.dpmm_param)\n",
    "        current_result = onlineVDP_suff_stat.online_vdpgmm_suffstat(z, init_prior, self.opts, move_dict_to_device(self.current_result, z.device))\n",
    "        self.current_result = current_result\n",
    "        self.current_suff_stat = current_result['sum_stat']\n",
    "        current_K = self.current_result['K']\n",
    "\n",
    "        self.calc_cluster_component_params()\n",
    "        \n",
    "    def calc_cluster_component_params(self):\n",
    "        # expected value of mean vector and covariance matrix of each gaussian component in the DPGMM\n",
    "\n",
    "        # means and covariance matrices\n",
    "        self.comp_mu = [self.current_result['hp_posterior']['m'][:,k] for k in range(self.current_result['K']-1)]\n",
    "        self.comp_var = [torch.inverse(self.current_result['hp_posterior']['lambda'][:,k]*self.current_result['hp_posterior']['W'][:,:,k]) for k in range(self.current_result['K']-1)]\n",
    "        print(\"Log: current K\", self.current_result['true_K']) # ignore the additional component\n",
    "        \n",
    "    # we use the entire set of lantent variables z to fit the DPGMM model, but when calculating the loss, we only use a batch of data\n",
    "    # the cluster_assignments function outputs the responsibility and the hard assignments of the data in the batch\n",
    "\n",
    "    # Backprop: calculate the loss function\n",
    "    # loss = reconstruction loss + KL divergence   \n",
    "    def loss_function(self,\n",
    "                    *args,\n",
    "                    batch_size,\n",
    "                    batch_idx,\n",
    "                    kl_weight) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2] # u(z|x)\n",
    "        log_var = args[3] # sigma(z|x)\n",
    "        z = args[4]  # batch_size * latent_dim\n",
    "\n",
    "        # reconstruction loss\n",
    "        recons_loss = F.mse_loss(recons, input, reduction='sum')\n",
    "\n",
    "        # calculate kl divergence\n",
    "        kld_weight = kl_weight # Account for the minibatch samples from the dataset\n",
    "        # kl_weight = batch_size/size of the entire training set,\n",
    "\n",
    "        #estimate the DPGMM\n",
    "        if len(self.current_result) == 0:\n",
    "\n",
    "            kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "            loss = recons_loss + kld_weight * kld_loss\n",
    "            return {'loss': loss, 'reconstruction_loss':recons_loss, 'kld_loss': kld_loss, 'z': z}\n",
    "        \n",
    "        else:\n",
    "            #prob_comps --> responsibilities, comps --> Z[n] cluster assignments \n",
    "            #soft assignment and hard assignment\n",
    "            prob_comps, comps = self.cluster_assignments(z.detach().cpu()) # soft assignment N*K real_K = q_of_z.shape[1]-1\n",
    "            #print(f\"num_q_of_z:{self.current_posterior['q_of_z'].shape[1]}\")\n",
    "            #_ , comps = torch.max(prob_comps, dim=1) # comps: hard assignment, size = K\n",
    "\n",
    "            # get a distribution of the latent variables\n",
    "            var = torch.exp(0.5 * log_var)**2\n",
    "            # batch_shape [batch_size], event_shape [latent_dim]\n",
    "            dist = torch.distributions.MultivariateNormal(loc=mu, covariance_matrix=torch.diag_embed(var))  # distribution of z obtained from the encoder\n",
    "\n",
    "            # get a distribution for each cluster\n",
    "            B, K = prob_comps.shape # batch_shape (number of samples in the current minibatch), number of active clusters\n",
    "            #print(\"Log: B, K\", B, K)\n",
    "            #print(\"Log: prob_comps\", prob_comps.shape)\n",
    "            kld = torch.zeros(B).to(mu.device)\n",
    "\n",
    "            for k in range(K):\n",
    "            # batch_shape [], event_shape [latent_dim]\n",
    "\n",
    "            # responsibility to the kth component\n",
    "                prob_k = prob_comps[:, k] # size = B, responsibility of the kth component\n",
    "                dist_k = torch.distributions.MultivariateNormal(loc=self.comp_mu[k].to(mu.device), covariance_matrix=torch.diag_embed(torch.diag(self.comp_var[k]).to(mu.device))) # use diagonal covariance matrix here\n",
    "\n",
    "            # batch_shape [batch_size], event_shape [latent_dim]\n",
    "\n",
    "            # mini_batch 中每个zi都服从一个对应的multivariate normal分布zi - N(u(zi|xi),sigma(zi|xi))), 所以一共batch_size个multivariate normal分布\n",
    "            # 它们分别与第k个component，即N(uk,sigmak)求KL-divergence，再取平均，得到batch的 KL loss\n",
    "\n",
    "                expanded_dist_k = dist_k.expand(dist.batch_shape) # 把第k个component复制batch_shape次，再以tensor的形式一起计算KL divergence\n",
    "\n",
    "                kld_k = torch.distributions.kl_divergence(dist, expanded_dist_k)   #  shape [batch_shape, ]\n",
    "\n",
    "                #print(f\"prob_k_shape:{prob_k.shape}, kld_k_shape:{kld_k.shape}\")\n",
    "\n",
    "                kld += prob_k.to(mu.device) * kld_k\n",
    "\n",
    "        # 最终的 KL loss是minibatch中每个sample的KL loss的平均\n",
    "        kld_loss = torch.mean(kld)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        loss = loss.to(input.device)\n",
    "        return {'loss': loss, 'reconstruction_loss':recons_loss, 'kld_loss': kld_loss, 'z': z, 'comps': comps}\n",
    "\n",
    "    def cluster_assignments(self, z):\n",
    "        # z: batch_size * latent_dim --> z.t(): latent_dim * batch_size\n",
    "        q_of_z, _, _ = onlineVDP_suff_stat.mk_q_of_z(z.t(), self.current_result['hp_posterior'], self.current_result['hp_prior'], self.opts)\n",
    "        # Here, responsibility is a 2D array of size N x K. here N is batch size, K active clusters\n",
    "        # Each entry resp[n, k] gives the probability that data atom n is assigned to cluster k under the posterior.\n",
    "        resp = q_of_z[:,:-1]\n",
    "        # To convert to hard assignments\n",
    "        Z = resp.argmax(axis=1)\n",
    "        return resp, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_dict_to_device(state_dict, device):\n",
    "    '''move all tensors in state_dict to the device'''\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            new_state_dict[k] = v.to(device)\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "def forward(model, input: Tensor, **kwargs) -> Tensor:\n",
    "    return model(input, **kwargs)\n",
    "    \n",
    "    \n",
    "def training_step(model, batch_samples, batch_labels, batch_idx, kl_weight):\n",
    "    curr_device = batch_samples.device\n",
    "\n",
    "    batch_size = batch_samples.size(0)\n",
    "    results = forward(model, batch_samples, labels = batch_labels)\n",
    "    train_loss = model.loss_function(*results, batch_size = batch_size, batch_idx = batch_idx,\n",
    "                                              kl_weight = kl_weight,\n",
    "                                              )\n",
    "    #for name, metric in train_loss.items():\n",
    "        #if \"loss\" in name:\n",
    "            #log(\"train_\" + name, metric.mean().item(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    train_loss.update({'labels': batch_labels})\n",
    "    #print(f\"train_loss:{train_loss}\")\n",
    "    return train_loss    # latent encoding\n",
    "\n",
    "def training_epoch_end(model, current_epoch, total_epochs, dpmm_init_epoch, outputs):\n",
    "    if current_epoch >= dpmm_init_epoch:\n",
    "        z = torch.cat([outputs[i]['z'] for i in range(0, len(outputs))])\n",
    "        model.fit_dpmm(z)\n",
    "    \n",
    "    if \"comps\" in outputs[0]:\n",
    "        comps = []\n",
    "        for i in range(0, len(outputs)):\n",
    "            comps.extend(outputs[i]['comps'])\n",
    "        comps = np.array(comps)\n",
    "        #comps = np.array([outputs[i]['comps'] for i in range(0, len(outputs))]).flatten()\n",
    "        labels = torch.cat([outputs[i]['labels'] for i in range(0, len(outputs))]).cpu()\n",
    "        #print(f\"labels:{labels}\")\n",
    "        #print(f\"comps:{comps}\")\n",
    "        acc, _ = unsupervised_clustering_accuracy(labels.numpy(), comps)\n",
    "        ars = adjusted_rand_score(labels.numpy(), comps)\n",
    "        nmi = normalized_mutual_info_score(labels.numpy(), comps)\n",
    "        #self.log(\"train_clustering_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"Number_of_DP_Comps\", int(self.model.bnp_model.obsModel.K), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        print(f\"train_clustering_acc:{acc}\", f\"train_ars:{ars}\", f\"train_nmi:{nmi}\")\n",
    "        print(f\"Number_of_DP_Comps:{model.current_result['true_K']}\")\n",
    "        Nk = model.current_result['sum_stat']['Nk'].flatten()\n",
    "        formatted_Nk = ', '.join(map(lambda x: f\"{x:.2f}\", Nk.numpy()))\n",
    "        print(f\"Nk = [{formatted_Nk}]\")\n",
    "\n",
    "        return comps, labels, acc\n",
    "    \n",
    "def validation_step(model, batch_samples, batch_labels, batch_idx, kl_weight):\n",
    "    batch_size = batch_samples.size(0)\n",
    "    results = forward(model, batch_samples, labels = batch_labels)\n",
    "    val_loss = model.loss_function(*results, batch_size = batch_size, batch_idx = batch_idx, kl_weight = kl_weight)\n",
    "                                        \n",
    "    val_loss.update({'labels': batch_labels})\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def validation_epoch_end(current_epoch, total_epochs, outputs):\n",
    "    \n",
    "    print(f\"====================== Validation Epoch: {current_epoch} ========================\")\n",
    "\n",
    "    if \"comps\" in outputs[0]:\n",
    "        comps = []\n",
    "        for i in range(0, len(outputs)):\n",
    "            comps.extend(outputs[i]['comps'])\n",
    "        comps = np.array(comps)\n",
    "        labels = torch.cat([outputs[i]['labels'] for i in range(0, len(outputs))]).cpu()\n",
    "        acc, _ = unsupervised_clustering_accuracy(labels.numpy(), comps)\n",
    "        ars = adjusted_rand_score(labels.numpy(), comps)\n",
    "        nmi = normalized_mutual_info_score(labels.numpy(), comps)\n",
    "\n",
    "        print(f\"valid_clustering_acc:{acc}\", f\"valid_ars:{ars}\", f\"valid_nmi:{nmi}\")\n",
    "        return acc\n",
    "        \n",
    "def test_epoch(current_epoch, total_epochs, outputs):\n",
    "\n",
    "    if \"comps\" in outputs[0]:\n",
    "        comps = []\n",
    "        for i in range(0, len(outputs)):\n",
    "            comps.extend(outputs[i]['comps'])\n",
    "        comps = np.array(comps)\n",
    "        labels = torch.cat([outputs[i]['labels'] for i in range(0, len(outputs))]).cpu()\n",
    "        acc, _ = unsupervised_clustering_accuracy(labels.numpy(), comps)\n",
    "        ars = adjusted_rand_score(labels.numpy(), comps)\n",
    "        nmi = normalized_mutual_info_score(labels.numpy(), comps)\n",
    "\n",
    "        print(f\"test_clustering_acc:{acc}\", f\"test_ars:{ars}\", f\"test_nmi:{nmi}\")\n",
    "\n",
    "        return acc\n",
    "        \n",
    "\n",
    "def unsupervised_clustering_accuracy(y: Union[np.ndarray, torch.Tensor], y_pred: Union[np.ndarray, torch.Tensor]) -> tuple:\n",
    "        \"\"\"Unsupervised Clustering Accuracy\n",
    "        \"\"\"\n",
    "        assert len(y_pred) == len(y)\n",
    "        u = np.unique(y)\n",
    "        n_true_clusters = len(u)\n",
    "        v = np.unique(y_pred)\n",
    "        n_pred_clusters = len(v)\n",
    "        map_u = dict(zip(u, range(n_true_clusters)))\n",
    "        map_v = dict(zip(v, range(n_pred_clusters)))\n",
    "        inv_map_u = {v: k for k, v in map_u.items()}\n",
    "        inv_map_v = {v: k for k, v in map_v.items()}\n",
    "        r = np.zeros((n_pred_clusters, n_true_clusters), dtype=np.int64)\n",
    "        for y_pred_, y_ in zip(y_pred, y):\n",
    "            if y_ in map_u:\n",
    "                r[map_v[y_pred_], map_u[y_]] += 1\n",
    "        reward_matrix  = np.concatenate((r, r, r), axis=1)\n",
    "        cost_matrix = reward_matrix.max() - reward_matrix\n",
    "        row_assign, col_assign = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "        # Construct optimal assignments matrix\n",
    "        row_assign = row_assign.reshape((-1, 1))  # (n,) to (n, 1) reshape\n",
    "        col_assign = col_assign.reshape((-1, 1))  # (n,) to (n, 1) reshape\n",
    "        assignments = np.concatenate((row_assign, col_assign), axis=1)\n",
    "        assignments = [[inv_map_v[x], inv_map_u[y%n_true_clusters]] for x, y in assignments]\n",
    "\n",
    "        optimal_reward = reward_matrix[row_assign, col_assign].sum() * 1.0\n",
    "        return optimal_reward / y_pred.size, assignments       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damage_detection_acc(y, y_pred):\n",
    "    y = y.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    \n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    most_tru = np.bincount(y).argmax()\n",
    "    #most_pre = 3\n",
    "    most_pre = np.bincount(y_pred).argmax()\n",
    "\n",
    "\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == most_tru and y_pred[i] != most_pre:\n",
    "            fp += 1\n",
    "        if y[i] != most_tru and y_pred[i] == most_pre:\n",
    "            fn += 1\n",
    "        dda = 1 - (fn + fp) / len(y)        \n",
    "    return dda, fn, fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================== Seed: 12345 ========================\n",
      "torch.Size([2000, 1000])\n",
      "cuda\n",
      "Log: opts {'algorithm': 'vdp', 'init_of_split': 'pc', 'initial_K': 1, 'do_sort': 1, 'do_greedy_split': 1, 'do_split': 0, 'do_merge': 1, 'get_q_of_z': 0, 'get_E_pi': 0, 'get_log_likelihood': 0, 'max_iter_merge': 10, 'max_iter_split': 3, 'max_merge': 10, 'max_split': 5, 'ite': 100, 'threshold': 1e-05, 'min_split_size': 2, 'alpha': tensor(100.), 'beta0': tensor(0.0100)}\n",
      "<bound method Module.state_dict of DP_VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=500, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=2000, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=2000, out_features=10, bias=True)\n",
      "  (fc_log_var): Linear(in_features=2000, out_features=10, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=2000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=500, out_features=1000, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_epoch 0/150: 100%|██████████| 50/50 [00:00<00:00, 132.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:2685.1493 reconstruction_loss:2684.9217 kl_loss:0.2277\n",
      "### greedy splitting ###\n",
      "free_energy not decreased after splitting\n",
      "F=18913;    Nk=[1600.00, 0.00];\n",
      "merge step is not implemented\n",
      "Log: current K 1\n",
      "====================== Validation Epoch: 0 ========================\n",
      "valid_clustering_acc:0.36 valid_ars:0.0 valid_nmi:0.0\n",
      "valid_loss:1304.0091 val_reconstruction_loss:1303.8858 val_kl_loss:0.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_epoch 1/150: 100%|██████████| 50/50 [00:00<00:00, 107.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:1325.8505 reconstruction_loss:1324.9156 kl_loss:0.9348\n",
      "test_clustering_acc:0.3 test_ars:0.0 test_nmi:0.0\n",
      "### greedy splitting ###\n",
      "free_energy not decreased after splitting\n",
      "F=20255;    Nk=[1572.58, 27.42, 0.00];\n",
      "### merge step ###\n",
      "less than 2 components\n",
      "Log: current K 1\n",
      "train_clustering_acc:0.2925 train_ars:0.0 train_nmi:0.0\n",
      "Number_of_DP_Comps:1\n",
      "Nk = [1600.00, 0.00]\n",
      "====================== Validation Epoch: 1 ========================\n",
      "valid_clustering_acc:0.36 valid_ars:0.0 valid_nmi:0.0\n",
      "valid_loss:1003.1912 val_reconstruction_loss:1001.8922 val_kl_loss:1.2990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_epoch 2/150: 100%|██████████| 50/50 [00:00<00:00, 101.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:1084.0857 reconstruction_loss:1079.1897 kl_loss:4.8960\n",
      "test_clustering_acc:0.3 test_ars:0.0 test_nmi:0.0\n",
      "### greedy splitting ###\n",
      "free_energy not decreased after splitting\n",
      "F=18647;    Nk=[1105.50, 494.50, 0.00];\n",
      "### merge step ###\n",
      "free_energy not decreased after merging\n",
      "F=18647;    Nk=[1105.50, 494.50, 0.00];\n",
      "Log: current K 2\n",
      "train_clustering_acc:0.2925 train_ars:0.0 train_nmi:0.0\n",
      "Number_of_DP_Comps:2\n",
      "Nk = [1105.50, 494.50, 0.00]\n",
      "====================== Validation Epoch: 2 ========================\n",
      "valid_clustering_acc:0.45 valid_ars:0.22881493123094065 valid_nmi:0.4480228631896178\n",
      "valid_loss:740.9144 val_reconstruction_loss:738.4902 val_kl_loss:2.4243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_epoch 3/150: 100%|██████████| 50/50 [00:00<00:00, 60.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:733.5058 reconstruction_loss:728.2951 kl_loss:5.2107\n",
      "test_clustering_acc:0.44 test_ars:0.2784653297069816 test_nmi:0.49928706049361626\n",
      "### greedy splitting ###\n",
      "free_energy not decreased after splitting\n",
      "F=12929;    Nk=[671.53, 312.86, 283.18, 168.10, 164.32, 0.00];\n",
      "### merge step ###\n",
      "free_energy not decreased after merging\n",
      "F=12929;    Nk=[671.53, 312.86, 283.18, 168.10, 164.32, 0.00];\n",
      "Log: current K 5\n",
      "train_clustering_acc:0.396875 train_ars:0.23792090683998424 train_nmi:0.45902908857839214\n",
      "Number_of_DP_Comps:5\n",
      "Nk = [671.53, 312.86, 283.18, 168.10, 164.32, 0.00]\n",
      "====================== Validation Epoch: 3 ========================\n",
      "valid_clustering_acc:0.625 valid_ars:0.38198725362096764 valid_nmi:0.6644173669500891\n",
      "valid_loss:536.1637 val_reconstruction_loss:532.9006 val_kl_loss:3.2631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current_epoch 4/150: 100%|██████████| 50/50 [00:01<00:00, 42.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss:593.7103 reconstruction_loss:588.2197 kl_loss:5.4906\n",
      "test_clustering_acc:0.605 test_ars:0.430445573481514 test_nmi:0.6963981561543838\n",
      "### greedy splitting ###\n",
      "free_energy not decreased after splitting\n",
      "F=3703.8;    Nk=[632.81, 315.00, 314.29, 167.00, 166.00, 4.90, 0.00];\n",
      "### merge step ###\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 175\u001b[0m\n\u001b[0;32m    172\u001b[0m         test_acc_epoch\u001b[38;5;241m.\u001b[39mappend(test_acc)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m train_loss_batch[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 175\u001b[0m     comps0, labels0, training_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpmm_init_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     training_acc_epoch\u001b[38;5;241m.\u001b[39mappend(training_acc)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m, in \u001b[0;36mtraining_epoch_end\u001b[1;34m(model, current_epoch, total_epochs, dpmm_init_epoch, outputs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m dpmm_init_epoch:\n\u001b[0;32m     33\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([outputs[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(outputs))])\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_dpmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     37\u001b[0m     comps \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[3], line 108\u001b[0m, in \u001b[0;36mDP_VAE.fit_dpmm\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m    106\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mt()\n\u001b[0;32m    107\u001b[0m init_prior \u001b[38;5;241m=\u001b[39m onlineVDP_suff_stat\u001b[38;5;241m.\u001b[39mmk_hp_prior(z, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdpmm_param)\n\u001b[1;32m--> 108\u001b[0m current_result \u001b[38;5;241m=\u001b[39m \u001b[43monlineVDP_suff_stat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_vdpgmm_suffstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmove_dict_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_result \u001b[38;5;241m=\u001b[39m current_result\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_suff_stat \u001b[38;5;241m=\u001b[39m current_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum_stat\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\OneDrive - Nanyang Technological University\\DP-VAE\\codes\\onlineVDP_suff_stat.py:1106\u001b[0m, in \u001b[0;36monline_vdpgmm_suffstat\u001b[1;34m(data, hp_prior, opts, previous_results)\u001b[0m\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown algorithm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_merge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m sum_stat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNk\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;66;03m# if there is more than one component, we can do merge step\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m     free_energy, hp_posterior, data, q_of_z, sum_stat \u001b[38;5;241m=\u001b[39m \u001b[43mdo_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_of_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_posterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_stat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfree_energy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmerge step is not implemented\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\OneDrive - Nanyang Technological University\\DP-VAE\\codes\\onlineVDP_suff_stat.py:837\u001b[0m, in \u001b[0;36mdo_merge\u001b[1;34m(data, q_of_z, hp_posterior, hp_prior, sum_stat, free_energy, opts)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m free_energy, hp_posterior, data, q_of_z, sum_stat\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# find the best pair of components to merge that minimize the free energy after merging\u001b[39;00m\n\u001b[1;32m--> 837\u001b[0m kA, kB, free_energy_merge_best, hp_posterior_merge, q_of_z_merge, sum_stat_merge \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_of_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_posterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_stat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;66;03m#my_disp(f'merging components {kA} and {kB} minimize free energy, the free energy after merging is {free_energy_merge_best.item():.5g}')\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m free_energy_decreased(free_energy, free_energy_merge_best, \u001b[38;5;241m0\u001b[39m, opts) \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iter_merge\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;66;03m# if the free energy does not decrease after merging the two components, reject the merge and stopt the merging process\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\OneDrive - Nanyang Technological University\\DP-VAE\\codes\\onlineVDP_suff_stat.py:815\u001b[0m, in \u001b[0;36mfind_best_merge\u001b[1;34m(data, q_of_z, hp_posterior, hp_prior, sum_stat, opts)\u001b[0m\n\u001b[0;32m    813\u001b[0m new_sum_stat \u001b[38;5;241m=\u001b[39m compute_summary_stats(data, new_q_of_z, opts)\n\u001b[0;32m    814\u001b[0m new_hp_posterior \u001b[38;5;241m=\u001b[39m mk_hp_posterior(hp_prior, new_sum_stat, opts)\n\u001b[1;32m--> 815\u001b[0m free_energy_merge, hp_posterior_merge, data, q_of_z_merge, sum_stat_merge \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_posterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_hp_posterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_sum_stat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sort\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    816\u001b[0m free_energy_merge_set\u001b[38;5;241m.\u001b[39mappend(free_energy_merge\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    817\u001b[0m merge_component_set\u001b[38;5;241m.\u001b[39mappend((kA, kB))\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\OneDrive - Nanyang Technological University\\DP-VAE\\codes\\onlineVDP_suff_stat.py:872\u001b[0m, in \u001b[0;36mupdate_posterior\u001b[1;34m(data, hp_posterior, hp_prior, sum_stat, opts, ite, do_sort)\u001b[0m\n\u001b[0;32m    870\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m#print(f\"sum_stat in update_posterior:{sum_stat}\")\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m new_free_energy, omega \u001b[38;5;241m=\u001b[39m \u001b[43mmk_free_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_posterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msum_stat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;66;03m#print(f\"new_free_energy:{new_free_energy}\")\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m#disp_status(new_free_energy, hp_posterior, opts)\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(torch\u001b[38;5;241m.\u001b[39mtensor(ite))\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m ite) \u001b[38;5;129;01mor\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misinf(torch\u001b[38;5;241m.\u001b[39mtensor(ite))\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m free_energy_decreased(free_energy, new_free_energy, \u001b[38;5;241m0\u001b[39m, opts)):\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\OneDrive - Nanyang Technological University\\DP-VAE\\codes\\onlineVDP_suff_stat.py:346\u001b[0m, in \u001b[0;36mmk_free_energy\u001b[1;34m(data, hp_posterior, hp_prior, sum_stat, opts, fc, omega)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m omega \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     fc \u001b[38;5;241m=\u001b[39m mk_E_log_q_p_eta(data, hp_posterior, hp_prior, sum_stat, opts) \u001b[38;5;66;03m# accounts for the parameters of the normal-Wishart distribution in the ELBO\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     omega \u001b[38;5;241m=\u001b[39m \u001b[43mmk_omega\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_posterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_prior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m N, K \u001b[38;5;241m=\u001b[39m omega\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvdp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\OneDrive - Nanyang Technological University\\DP-VAE\\codes\\onlineVDP_suff_stat.py:178\u001b[0m, in \u001b[0;36mmk_omega\u001b[1;34m(data, hp_posterior, hp_prior, opts)\u001b[0m\n\u001b[0;32m    170\u001b[0m E_log_p_of_z_given_other_z_k \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    171\u001b[0m     torch\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mpsi(hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, k])\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mpsi(torch\u001b[38;5;241m.\u001b[39msum(hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m][:, k], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mpsi(hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m, :k]) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mpsi(torch\u001b[38;5;241m.\u001b[39msum(hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :k], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    176\u001b[0m Precision \u001b[38;5;241m=\u001b[39m hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :, k] \u001b[38;5;241m*\u001b[39m hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][k]  \u001b[38;5;66;03m# D*D\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m E_log_p_of_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m D \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mpi)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdetln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp_posterior\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minv_W\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m psi_sum[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m D \u001b[38;5;241m/\u001b[39m hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m,k] \u001b[38;5;66;03m# 1*1 \u001b[39;00m\n\u001b[0;32m    180\u001b[0m d \u001b[38;5;241m=\u001b[39m data \u001b[38;5;241m-\u001b[39m hp_posterior[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m][:, k]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,N) \u001b[38;5;66;03m# D*N\u001b[39;00m\n\u001b[0;32m    182\u001b[0m E_log_p_of_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m d \u001b[38;5;241m*\u001b[39m (Precision \u001b[38;5;241m@\u001b[39m d), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m E_log_p_of_x \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "seeds = [12345,1234,600,100,10,12345*2,400,20000]\n",
    "torch.manual_seed(12345)\n",
    "# initial_state = torch.get_rng_state()\n",
    "# initial_cuda_state = torch.cuda.get_rng_state()\n",
    "performance_seed = []\n",
    "total_epochs = 150\n",
    "\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    print(f\"====================== Seed: {seed} ========================\")\n",
    "    \n",
    "    #torch.set_rng_state(initial_state)\n",
    "\n",
    "    #torch.cuda.set_rng_state(initial_cuda_state)\n",
    "\n",
    "    torch.manual_seed(seed)  # ensure reproducible results\n",
    "\n",
    "    # # # Read data from CSV\n",
    "    # features = pd.read_csv(\"TF_numerical1.csv\")\n",
    "    # features = features.astype(\"float32\")\n",
    "    # # Convert DataFrame to PyTorch tensors\n",
    "    # X = torch.tensor(features.values[:600,:])\n",
    "    # X = X.transpose(0,1)\n",
    "\n",
    "    '''use a numerical dataset to test the model\n",
    "    label  damaged_floor     damage_extent         number_of_samples\n",
    "    0           0            0%                     600\n",
    "    1           1            5%                     200\n",
    "    2           1            10%                    200\n",
    "    3           2,4          10%, 10%               200\n",
    "    4           1,3,5        10%,15%,20%            200\n",
    "    5           2,4,6        15%,20%,25%            200\n",
    "    6           1,3,5,7      10%,15%,20%,25%        200\n",
    "    7           1,2,4,6,8    10%,15%,20%,25%,30%    200\n",
    "\n",
    "    '''\n",
    "   # Read data from CSV\n",
    "    features = pd.read_csv(\"toy_dataset.csv\")\n",
    "    features = features.astype(\"float32\")\n",
    "    # # # Convert DataFrame to PyTorch tensors\n",
    "    X = torch.tensor(features.values[:,:1000])\n",
    "\n",
    "    input_dim = X.shape[1]\n",
    "    print(X.shape)\n",
    "\n",
    "    y0 = torch.tensor(features.values[:,1000])\n",
    "    y0 = y0.unsqueeze(1)\n",
    "    y = [int(_) for _ in y0] #+ 0.01*y0.mean()*torch.randn(y0.shape[0], 1)\n",
    "    y = torch.tensor(y)\n",
    "    # Split the data into training and test sets\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = int(0.1 * len(dataset))\n",
    "    valid_size = len(dataset) - train_size - test_size\n",
    "    train_dataset, test_dataset, valid_dataset = random_split(dataset, [train_size, test_size, valid_size])\n",
    "\n",
    "    # Accessing the tensors from train_dataset and test_dataset\n",
    "    x_train, y_train = train_dataset[:]\n",
    "    x_valid, y_valid = valid_dataset[:]\n",
    "    # Optionally, you can also access the test set\n",
    "    x_test, y_test = test_dataset[:]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    dp_alpha = 10.0\n",
    "    latent_dim = 10\n",
    "    model_params = dict(\n",
    "        input_dim = input_dim,\n",
    "        latent_dim = latent_dim,\n",
    "        hidden_layers = [500, 500, 2000],\n",
    "        dpmm_param = dict(\n",
    "            alpha=torch.tensor(dp_alpha),\n",
    "            beta0=torch.tensor(0.01),\n",
    "            max_iter_merge = 10,\n",
    "            max_iter_split = 3,\n",
    "            max_merge = 10,\n",
    "            max_split = 5,\n",
    "            ite = 100,\n",
    "        )\n",
    "    )\n",
    "    # MLP format\n",
    "    model = DP_VAE(**model_params).to(device)\n",
    "    print(model.state_dict)\n",
    "        # plt.plot(y)\n",
    "    # plt.show()\n",
    "\n",
    "    # training phase\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 5e-5, weight_decay = 0)\n",
    "    kl_weight = 1 #batch_size/len(train_loader.dataset)\n",
    "    # total_epoch拟合DPGMM-训练vae 这一循环的epoch\n",
    "    dpmm_init_epoch = 0\n",
    "\n",
    "    training_loss_epoch = []\n",
    "    recon_loss_epoch = []\n",
    "    kl_loss_epoch = []\n",
    "\n",
    "    val_loss_epoch = []\n",
    "    val_recon_loss_epoch = []\n",
    "    val_kl_loss_epoch = []\n",
    "\n",
    "    training_acc_epoch = []\n",
    "    val_acc_epoch = []\n",
    "    test_acc_epoch = []\n",
    "\n",
    "    y_ground_truth = torch.tensor([])\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"current_epoch {epoch}/{total_epochs}\")\n",
    "            # training_loss_batch只记录training loss (total loss)\n",
    "        training_loss_batch = []\n",
    "        recon_loss_batch = []\n",
    "        kl_loss_batch = []\n",
    "            # train_loss_batch = [] 将一个epoch中所有mini_batch数据训练得出的loss存入一个列表, 包括 total loss， reconstruction loss， kl loss,‘z’,'components'\n",
    "        train_loss_batch = []\n",
    "        batch_idx = 0\n",
    "        for batch_samples, batch_labels in pbar:\n",
    "\n",
    "            if epoch == total_epochs - 1:\n",
    "                y_ground_truth = torch.cat([y_ground_truth, batch_labels])\n",
    "\n",
    "            batch_samples = batch_samples.to(device)\n",
    "\n",
    "            #train_loss = model.loss_function(*results, # M_N = self.params['batch_size']/ self.num_train_imgs,\n",
    "                                                    #M_N=1/train_size, optimizer_idx = 0, batch_idx = batch_idx, device = 'cpu')\n",
    "\n",
    "            train_loss = training_step(model, batch_samples, batch_labels, batch_idx, kl_weight)\n",
    "            train_loss_batch.append(train_loss)\n",
    "            \n",
    "\n",
    "            # total loss in of a minibatch\n",
    "            training_loss = train_loss['loss']\n",
    "            training_loss_batch.append(train_loss['loss'].item())\n",
    "            recon_loss_batch.append(train_loss['reconstruction_loss'].item())\n",
    "            kl_loss_batch.append(kl_weight*train_loss['kld_loss'].item())\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            training_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "\n",
    "        training_loss_epoch.append(np.mean(training_loss_batch))\n",
    "        recon_loss_epoch.append(np.mean(recon_loss_batch))\n",
    "        kl_loss_epoch.append(np.mean(kl_loss_batch))\n",
    "\n",
    "        print(f\"training_loss:{np.mean(training_loss_batch):.4f}\", f\"reconstruction_loss:{np.mean(recon_loss_batch):.4f}\", f\"kl_loss:{np.mean(kl_loss_batch):.4f}\")\n",
    "\n",
    "        if \"comps\" in train_loss_batch[0]:\n",
    "            with torch.no_grad():\n",
    "                test_loss_batch = []\n",
    "                batch_idx_test = 0\n",
    "                \n",
    "                for batch_samples, batch_labels in test_loader:\n",
    "                    batch_samples = batch_samples.to(device)\n",
    "                    test_loss = validation_step(model, batch_samples, batch_labels, batch_idx_test, kl_weight = 1)# batch_size/len(valid_loader.dataset))\n",
    "                    test_loss_batch.append(test_loss)\n",
    "                    batch_idx_test += 1\n",
    "\n",
    "                test_acc = test_epoch(epoch, total_epochs, test_loss_batch)\n",
    "                test_acc_epoch.append(test_acc)\n",
    "\n",
    "        if \"comps\" in train_loss_batch[0]:\n",
    "            comps0, labels0, training_acc = training_epoch_end(model, epoch, total_epochs, dpmm_init_epoch, train_loss_batch)\n",
    "            training_acc_epoch.append(training_acc)\n",
    "        else:\n",
    "            training_epoch_end(model, epoch, total_epochs, dpmm_init_epoch, train_loss_batch)\n",
    "\n",
    "        #if epoch > 0:\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_loss_batch = []\n",
    "            valid_loss_batch = []\n",
    "            val_recon_loss_batch = []\n",
    "            val_kl_loss_batch = []\n",
    "            batch_idx_val = 0\n",
    "\n",
    "            for batch_samples, batch_labels in valid_loader:\n",
    "\n",
    "                batch_samples = batch_samples.to(device)\n",
    "                val_loss = validation_step(model, batch_samples, batch_labels, batch_idx_val, kl_weight = 1) # batch_size/len(valid_loader.dataset))\n",
    "                val_loss_batch.append(val_loss)\n",
    "\n",
    "                valid_loss_batch.append(val_loss['loss'].item())\n",
    "                val_recon_loss_batch.append(val_loss['reconstruction_loss'].item())\n",
    "                val_kl_loss_batch.append(kl_weight*val_loss['kld_loss'].item())\n",
    "\n",
    "                batch_idx_val += 1\n",
    "            \n",
    "            val_loss_epoch.append(np.mean(valid_loss_batch))\n",
    "            val_recon_loss_epoch.append(np.mean(val_recon_loss_batch))\n",
    "            val_kl_loss_epoch.append(np.mean(val_kl_loss_batch))\n",
    "            \n",
    "            val_acc = validation_epoch_end(epoch, total_epochs, val_loss_batch)\n",
    "            val_acc_epoch.append(val_acc)\n",
    "            print(f\"valid_loss:{np.mean(valid_loss_batch):.4f}\", f\"val_reconstruction_loss:{np.mean(val_recon_loss_batch):.4f}\", f\"val_kl_loss:{np.mean(val_kl_loss_batch):.4f}\")\n",
    "\n",
    "\n",
    "    # performance on the training set\n",
    "    np.random.seed(seed)\n",
    "    latent_variables = torch.tensor([])\n",
    "    for i in range(len(train_loss_batch)): \n",
    "        latent_variables = torch.cat([latent_variables,train_loss_batch[i]['z'].cpu()],dim=0)\n",
    "        \n",
    "    print(latent_variables.shape)\n",
    "\n",
    "    preds = model.current_result['hard_assign']\n",
    "\n",
    "    #  clustering accuracy of the training set\n",
    "    acc_training, _ = unsupervised_clustering_accuracy(y_ground_truth.numpy().astype(int), preds.numpy())\n",
    "    ars_training = adjusted_rand_score(y_ground_truth.numpy().astype(int), preds.numpy())\n",
    "    nmi_training = normalized_mutual_info_score(y_ground_truth.numpy().astype(int), preds.numpy())\n",
    "\n",
    "\n",
    "    # validation set performance\n",
    "    y_valid = torch.tensor([])\n",
    "    for batch_samples, batch_labels in valid_loader:\n",
    "        y_valid = torch.cat([y_valid, batch_labels], dim=0)\n",
    "\n",
    "    latent_val = torch.tensor([])\n",
    "    preds_val = torch.tensor([])\n",
    "    for i in range(len(val_loss_batch)): \n",
    "        latent_val = torch.cat([latent_val,val_loss_batch[i]['z'].cpu()],dim=0)\n",
    "        preds_val = torch.cat([preds_val,val_loss_batch[i]['comps'].cpu()],dim=0)\n",
    "\n",
    "    acc_valid, _ = unsupervised_clustering_accuracy(y_valid.numpy().astype(int), preds_val.numpy())\n",
    "    ars_valid = adjusted_rand_score(y_valid.numpy().astype(int), preds_val.numpy())\n",
    "    nmi_valid = normalized_mutual_info_score(y_valid.numpy().astype(int), preds_val.numpy())    \n",
    "\n",
    "    # test set performance\n",
    "    y_test = torch.tensor([])\n",
    "    for batch_samples, batch_labels in test_loader:\n",
    "        y_test = torch.cat([y_test, batch_labels], dim=0)\n",
    "\n",
    "    latent_test = torch.tensor([])\n",
    "    preds_test = torch.tensor([])\n",
    "    for i in range(len(test_loss_batch)): \n",
    "        latent_test = torch.cat([latent_test,test_loss_batch[i]['z'].cpu()],dim=0)\n",
    "        preds_test = torch.cat([preds_test,test_loss_batch[i]['comps'].cpu()],dim=0)\n",
    "\n",
    "\n",
    "    acc_test, assignment_test = unsupervised_clustering_accuracy(y_test.numpy().astype(int), preds_test.numpy())\n",
    "    ars_test = adjusted_rand_score(y_test.numpy().astype(int), preds_test.numpy())\n",
    "    nmi_test = normalized_mutual_info_score(y_test.numpy().astype(int), preds_test.numpy())\n",
    "    dda_test = damage_detection_acc(y_test.numpy().astype(int), preds_test.numpy())\n",
    "    print(acc_test, ars_test, nmi_test, dda_test)\n",
    "\n",
    "\n",
    "    # performance on the test set after mapping\n",
    "    mapping = {}\n",
    "    try:\n",
    "        for i,j in assignment_test:\n",
    "            mapping[i] = j\n",
    "        test_mapped_labels = [mapping[i] for i in preds_test.numpy()]\n",
    "    except:\n",
    "        test_mapped_labels = y_test.numpy().astype(int)\n",
    "\n",
    "    #acc_test, _ = unsupervised_clustering_accuracy(test_mapped_labels, preds_test.numpy())\n",
    "    ars_test_mapped = adjusted_rand_score(test_mapped_labels, preds_test.numpy())\n",
    "    nmi_test_mapped = normalized_mutual_info_score(test_mapped_labels, preds_test.numpy())\n",
    "    dda_test_mapped = damage_detection_acc(np.array(test_mapped_labels), preds_test.numpy())\n",
    "    print(acc_test, ars_test, nmi_test, dda_test_mapped)\n",
    "\n",
    "\n",
    "    metrics = dict(\n",
    "    test_unsupervised_accuracy = acc_test,\n",
    "    damage_detection_accuracy = dda_test[0],\n",
    "    false_negative = dda_test[1],\n",
    "    false_positive = dda_test[2],\n",
    "    nmi=normalized_mutual_info_score(preds_test.numpy(), y_test.numpy().astype(int)),\n",
    "    homogeneity_score=homogeneity_score(preds_test.numpy(), y_test.numpy().astype(int)),\n",
    "    completeness_score=completeness_score(preds_test.numpy(), y_test.numpy().astype(int)),\n",
    "    v_measure_score=v_measure_score(preds_test.numpy(), y_test.numpy().astype(int)),\n",
    "    adjusted_rand_score=adjusted_rand_score(preds_test.numpy(), y_test.numpy().astype(int)),\n",
    "    )\n",
    "\n",
    "    metrics_mapped = dict(\n",
    "    test_unsupervised_accuracy = acc_test,\n",
    "    damage_detection_accuracy = dda_test_mapped[0],\n",
    "    false_negative = dda_test_mapped[1],\n",
    "    false_positive = dda_test_mapped[2],\n",
    "    nmi=normalized_mutual_info_score(preds_test.numpy(), test_mapped_labels),\n",
    "    homogeneity_score=homogeneity_score(preds_test.numpy(), test_mapped_labels ),\n",
    "    completeness_score=completeness_score(preds_test.numpy(), test_mapped_labels ),\n",
    "    v_measure_score=v_measure_score(preds_test.numpy(), test_mapped_labels ),\n",
    "    adjusted_rand_score=adjusted_rand_score(preds_test.numpy(), test_mapped_labels ),\n",
    "    embeddings = latent_test,\n",
    "    test_preds=preds_test.numpy(),\n",
    "    test_labels=y_test.numpy().astype(int),\n",
    "    test_mapped_labels=test_mapped_labels\n",
    "    )\n",
    "\n",
    "    performance_ = dict(seed = seed, metrics = metrics, mapped_metrics = metrics_mapped)\n",
    "    performance_seed.append(performance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9713897688624954\n",
      "0.9725705164869517\n",
      "0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MLF\\anaconda3\\envs\\DP_VAE\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\MLF\\anaconda3\\envs\\DP_VAE\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 't-SNE Visualization of the latent space')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = performance_seed[0]['mapped_metrics']\n",
    "print(adjusted_rand_score(a['test_preds'], a['test_labels']))\n",
    "print(adjusted_rand_score(a['test_preds'], a['test_mapped_labels']))\n",
    "\n",
    "test_acc,_ = unsupervised_clustering_accuracy(a['test_labels'], a['test_preds'])\n",
    "\n",
    "print(test_acc)\n",
    "embeddings = a['embeddings'].detach().numpy()\n",
    "tsne = TSNE(n_components=2, random_state=12345)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "plt.figure(figsize=(6, 5))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=a['test_preds'], cmap='tab10', s=10)\n",
    "plt.colorbar(scatter, ticks=range(15), label='Class Labels')\n",
    "plt.title('t-SNE Visualization of the latent space')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 100.0\n",
      "seed:12345\n",
      "metrics:{'test_unsupervised_accuracy': 0.99, 'damage_detection_accuracy': 1.0, 'false_negative': 0, 'false_positive': 0, 'nmi': 0.9720453552573621, 'homogeneity_score': 0.9545133949573635, 'completeness_score': 0.9902334003432308, 'v_measure_score': 0.9720453552573621, 'adjusted_rand_score': 0.9713897688624954}\n",
      "alpha = 100.0\n",
      "seed:1234\n",
      "metrics:{'test_unsupervised_accuracy': 1.0, 'damage_detection_accuracy': 0.995, 'false_negative': 0, 'false_positive': 1, 'nmi': 0.9859729110058458, 'homogeneity_score': 0.9723338969019689, 'completeness_score': 1.0, 'v_measure_score': 0.9859729110058456, 'adjusted_rand_score': 0.9829126263611379}\n",
      "alpha = 100.0\n",
      "seed:600\n",
      "metrics:{'test_unsupervised_accuracy': 1.0, 'damage_detection_accuracy': 1.0, 'false_negative': 0, 'false_positive': 0, 'nmi': 0.9950108122302619, 'homogeneity_score': 0.9900711613011278, 'completeness_score': 1.0, 'v_measure_score': 0.9950108122302618, 'adjusted_rand_score': 0.9962676274398423}\n",
      "alpha = 100.0\n",
      "seed:100\n",
      "metrics:{'test_unsupervised_accuracy': 1.0, 'damage_detection_accuracy': 1.0, 'false_negative': 0, 'false_positive': 0, 'nmi': 0.9949189191872018, 'homogeneity_score': 0.9898892121047804, 'completeness_score': 1.0, 'v_measure_score': 0.9949189191872019, 'adjusted_rand_score': 0.9954020390664942}\n",
      "alpha = 100.0\n",
      "seed:10\n",
      "metrics:{'test_unsupervised_accuracy': 0.985, 'damage_detection_accuracy': 0.99, 'false_negative': 0, 'false_positive': 2, 'nmi': 0.9699315349546875, 'homogeneity_score': 0.9536152463623617, 'completeness_score': 0.9868158840814268, 'v_measure_score': 0.9699315349546875, 'adjusted_rand_score': 0.9647291166969755}\n",
      "alpha = 100.0\n",
      "seed:24690\n",
      "metrics:{'test_unsupervised_accuracy': 1.0, 'damage_detection_accuracy': 1.0, 'false_negative': 0, 'false_positive': 0, 'nmi': 0.9950248645314971, 'homogeneity_score': 0.9900989879392718, 'completeness_score': 0.9999999999999999, 'v_measure_score': 0.9950248645314971, 'adjusted_rand_score': 0.9962614660076194}\n",
      "alpha = 100.0\n",
      "seed:400\n",
      "metrics:{'test_unsupervised_accuracy': 0.98, 'damage_detection_accuracy': 0.985, 'false_negative': 0, 'false_positive': 3, 'nmi': 0.9507444550180372, 'homogeneity_score': 0.9291079998746522, 'completeness_score': 0.9734126476167501, 'v_measure_score': 0.9507444550180372, 'adjusted_rand_score': 0.9462141757259834}\n",
      "alpha = 100.0\n",
      "seed:20000\n",
      "metrics:{'test_unsupervised_accuracy': 0.99, 'damage_detection_accuracy': 0.985, 'false_negative': 1, 'false_positive': 2, 'nmi': 0.9612109486143653, 'homogeneity_score': 0.9400840379885951, 'completeness_score': 0.9833092785573133, 'v_measure_score': 0.9612109486143652, 'adjusted_rand_score': 0.9531693042955081}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(performance_seed)):\n",
    "    print(f\"alpha = {dp_alpha}\")\n",
    "    print(f\"seed:{performance_seed[i]['seed']}\")\n",
    "    print(f\"metrics:{performance_seed[i]['metrics']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name\n",
    "output_file = 'output_alpha100_dda.txt'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_file, 'w') as f:\n",
    "    for i in range(len(performance_seed)):\n",
    "        test_preds = performance_seed[i]['mapped_metrics']['test_preds']\n",
    "        unique_values = np.unique(test_preds)\n",
    "        K = len(unique_values)\n",
    "        f.write(f\"alpha: {dp_alpha}\\n\")\n",
    "        f.write(f\"num_clusters: {K}\\n\")\n",
    "        f.write(f\"seed: {performance_seed[i]['seed']}\\n\")\n",
    "        f.write(f\"metrics: {performance_seed[i]['metrics']}\\n\")\n",
    "        a = list(performance_seed[i]['mapped_metrics'].items())[:len(performance_seed[i]['metrics'])]\n",
    "        f.write(f\"mapped_metrics: {a}\\n\")\n",
    "        f.write(\"\\n\")  # Add a newline for better readability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
